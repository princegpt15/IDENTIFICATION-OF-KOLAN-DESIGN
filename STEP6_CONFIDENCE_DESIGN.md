# STEP 6: CONFIDENCE SCORE GENERATION
## Comprehensive Design Document

**Author:** ML Engineering Team  
**Date:** December 28, 2025  
**Project:** Kolam Pattern Classification System  
**Version:** 1.0

---

## 1. DEFINITION: WHAT IS CONFIDENCE?

### 1.1 Conceptual Definition
In our hybrid Kolam classification system, **confidence** is a quantitative measure (0-100%) that represents the system's certainty that a predicted class label is correct. It combines:

1. **Statistical certainty** from the neural network's learned patterns
2. **Logical certainty** from domain-specific geometric rules
3. **Consistency checks** between different signal sources

### 1.2 Why Confidence is Required

| Requirement | Rationale |
|-------------|-----------|
| **Human-in-the-loop decision making** | Low-confidence predictions should trigger expert review |
| **Automated filtering** | High-confidence predictions can be auto-accepted |
| **Trust and transparency** | Users need to understand when the system is uncertain |
| **Error prevention** | Prevents propagation of incorrect classifications |
| **System improvement** | Low-confidence cases identify areas for model improvement |
| **Academic rigor** | Demonstrates awareness of model limitations |

### 1.3 Confidence vs Probability
âš ï¸ **Critical Distinction:**
- **CNN Probability**: P(class|features) â€” purely statistical, can be overconfident
- **Confidence Score**: Calibrated belief that prediction is correct â€” considers multiple evidence sources

---

## 2. CONFIDENCE CONTRIBUTORS

Our system aggregates confidence from three independent sources:

### 2.1 Source 1: CNN Softmax Probability
```
P_cnn = max(softmax(logits))
```

**Characteristics:**
- âœ… Captures learned visual patterns
- âœ… Provides probabilistic interpretation
- âš ï¸ Can be overconfident on out-of-distribution samples
- âš ï¸ Sensitive to adversarial patterns

**Example:**
```
Probabilities: [0.72, 0.15, 0.08, 0.05]
P_cnn = 0.72 (72% for Pulli Kolam)
```

### 2.2 Source 2: Rule-Based Validation Score
```
S_rule = Î£(w_i Ã— rule_i) / Î£(w_i)
```

**Characteristics:**
- âœ… Encodes domain expertise
- âœ… Interpretable geometric constraints
- âœ… Catches impossible configurations
- âš ï¸ Limited by handcrafted feature quality
- âš ï¸ May be too rigid for edge cases

**Example:**
```
Passed rules: dot_count (w=1.0), grid_regularity (w=1.0)
Failed rules: dot_spacing_std (w=0.7)
S_rule = (1.0 + 1.0) / (1.0 + 1.0 + 0.7) = 0.741
```

### 2.3 Source 3: Prediction Consistency (Entropy-based)
```
H = -Î£ p_i Ã— log(p_i)  [normalized entropy]
Consistency = 1 - (H / H_max)
```

**Characteristics:**
- âœ… Measures decisiveness of CNN
- âœ… Low when probability is spread uniformly
- âœ… High when one class dominates
- âœ… No additional feature extraction needed

**Example:**
```
Case A: [0.95, 0.02, 0.02, 0.01] â†’ H=0.26 â†’ Consistency=0.94 (decisive)
Case B: [0.40, 0.30, 0.20, 0.10] â†’ H=1.28 â†’ Consistency=0.38 (uncertain)
```

---

## 3. ALTERNATIVE CONFIDENCE FORMULATIONS

### 3.1 Alternative 1: Simple Weighted Average
```
C_simple = Î± Ã— P_cnn + Î² Ã— S_rule
where Î± + Î² = 1
```

**Pros:**
- Simple and interpretable
- Easy to tune weights
- Fast computation

**Cons:**
- Ignores prediction consistency
- Treats all sources equally regardless of context
- No penalty for conflicting signals

### 3.2 Alternative 2: Multiplicative Fusion
```
C_multi = (P_cnn)^Î± Ã— (S_rule)^Î² Ã— (Consistency)^Î³
where Î± + Î² + Î³ = 1
```

**Pros:**
- Strong penalty when any source is weak
- Encourages agreement across all sources
- Naturally handles conflicting signals

**Cons:**
- Too harsh â€” one weak signal destroys confidence
- Difficult to interpret mathematically
- Sensitive to zero values

### 3.3 Alternative 3: Adaptive Weighted Fusion with Entropy Penalty
```
C_adaptive = [Î± Ã— P_cnn + Î² Ã— S_rule] Ã— Consistency^Î³
where Î± + Î² = 1, Î³ controls entropy sensitivity
```

**Pros:**
- Combines additive and multiplicative benefits
- Entropy penalty reduces overconfidence
- Interpretable component contributions
- Handles conflicts gracefully

**Cons:**
- Slightly more complex
- Requires tuning Î³ parameter

---

## 4. FINAL CHOSEN FORMULATION

### 4.1 Formula
```python
# Step 1: Base confidence (weighted average)
C_base = Î± Ã— P_cnn + Î² Ã— S_rule

# Step 2: Entropy penalty
H_norm = -Î£(p_i Ã— log(p_i)) / log(n_classes)  # 0 to 1
Consistency = 1 - H_norm

# Step 3: Final confidence with entropy adjustment
C_final = C_base Ã— (1 - Î³ Ã— H_norm)

# Expressed alternatively:
C_final = (Î± Ã— P_cnn + Î² Ã— S_rule) Ã— (1 - Î³ Ã— H_norm)
```

### 4.2 Default Parameters
```python
Î± = 0.65  # CNN weight (primary signal)
Î² = 0.35  # Rule weight (validation signal)
Î³ = 0.20  # Entropy penalty strength
```

### 4.3 Justification

| Criterion | Why This Works |
|-----------|----------------|
| **Interpretability** | Each term has clear meaning: base consensus Ã— decisiveness |
| **Balanced penalties** | Entropy reduces but doesn't destroy confidence |
| **Domain alignment** | CNN leads, rules validate, entropy calibrates |
| **Robustness** | Performs well even when one source is noisy |
| **Tunability** | Three independent knobs for system behavior |

### 4.4 Mathematical Properties

**Property 1: Boundedness**
```
0 â‰¤ C_final â‰¤ 1  (always holds)
```

**Property 2: Agreement Bonus**
```
If P_cnn â‰ˆ S_rule â‰ˆ 1 and H_norm â‰ˆ 0:
C_final â‰ˆ 1 Ã— 1 = 1 (maximum confidence)
```

**Property 3: Conflict Penalty**
```
If P_cnn = 0.9, S_rule = 0.3:
C_base = 0.65Ã—0.9 + 0.35Ã—0.3 = 0.69
If H_norm = 0.5 (moderate uncertainty):
C_final = 0.69 Ã— (1 - 0.2Ã—0.5) = 0.62 (reduced)
```

---

## 5. CONFIDENCE THRESHOLDS

### 5.1 Threshold Design Philosophy
Thresholds should balance:
- **Automation rate** (higher thresholds = more human review)
- **Error rate** (lower thresholds = more mistakes)
- **Context sensitivity** (critical applications need higher thresholds)

### 5.2 Recommended Thresholds

| Level | Range | Action | Use Case |
|-------|-------|--------|----------|
| **Very High** | 90-100% | Auto-accept | Production deployment, batch processing |
| **High** | 75-90% | Auto-accept with logging | Normal operations |
| **Medium** | 60-75% | âš ï¸ Flag for review | Border cases, manual QA |
| **Low** | 40-60% | ðŸš¨ Require human verification | Ambiguous patterns |
| **Very Low** | 0-40% | âŒ Reject / manual classification | Out-of-distribution, corrupted images |

### 5.3 Threshold Adjustment Rules

**Conservative Mode** (minimize false positives):
```
High threshold: 85%+
Medium threshold: 70-85%
```

**Aggressive Mode** (maximize automation):
```
High threshold: 70%+
Medium threshold: 55-70%
```

### 5.4 Context-Aware Thresholds

For **critical applications** (museum cataloging, research datasets):
```python
if application_type == "critical":
    threshold_high = 90
    threshold_medium = 80
```

For **screening applications** (initial sorting, large-scale processing):
```python
if application_type == "screening":
    threshold_high = 70
    threshold_medium = 55
```

---

## 6. CONFIDENCE CALIBRATION

### 6.1 Calibration Techniques

**Technique 1: Temperature Scaling**
```python
# Applied to CNN logits before softmax
P_calibrated = softmax(logits / T)
where T > 1 reduces overconfidence
```

**Technique 2: Platt Scaling**
```python
# Post-hoc calibration
P_calibrated = sigmoid(a Ã— P + b)
where a, b learned on validation set
```

**Technique 3: Isotonic Regression**
```python
# Non-parametric calibration
P_calibrated = isotonic_function(P)
learns monotonic mapping from validation data
```

### 6.2 Validation Approach

**Step 1:** Collect predictions on validation set with true labels

**Step 2:** Bin predictions by confidence range

**Step 3:** Calculate actual accuracy per bin

**Step 4:** Check calibration:
```
Ideal: Confidence â‰ˆ Accuracy for each bin
Overconfident: Confidence > Accuracy
Underconfident: Confidence < Accuracy
```

### 6.3 Expected Calibration Error (ECE)
```python
ECE = Î£ (|bin_accuracy - bin_confidence| Ã— bin_size / total_samples)
```

**Target:** ECE < 0.05 (5% miscalibration)

---

## 7. EXPLAINABILITY OUTPUT

### 7.1 Components to Report

Every prediction includes:

```python
explanation = {
    # Final output
    "predicted_class": "Pulli Kolam",
    "confidence": 78.5,
    "confidence_level": "HIGH",
    "action": "AUTO_ACCEPT",
    
    # Component contributions
    "components": {
        "cnn_probability": 0.82,
        "cnn_contribution": 0.533,  # Î± Ã— P_cnn
        "rule_score": 0.74,
        "rule_contribution": 0.259,  # Î² Ã— S_rule
        "base_confidence": 0.792,   # sum of above
        "entropy_penalty": 0.035,
        "final_confidence": 0.785
    },
    
    # Detailed breakdowns
    "cnn_details": {
        "all_probabilities": {
            "Pulli Kolam": 0.82,
            "Chukku Kolam": 0.10,
            "Line Kolam": 0.05,
            "Freehand Kolam": 0.03
        },
        "entropy": 0.565,
        "decisiveness": "HIGH"
    },
    
    "rule_details": {
        "passed_rules": ["dot_count", "grid_regularity", "dot_density"],
        "failed_rules": ["dot_spacing_std"],
        "warnings": ["High spacing variance detected"],
        "rule_score_breakdown": {...}
    },
    
    # Reasoning
    "reasoning": [
        "CNN strongly predicts Pulli Kolam (82%)",
        "Rules validate prediction (74% rule compliance)",
        "Low entropy indicates decisive prediction",
        "Minor spacing irregularity noted but not critical",
        "Overall: HIGH confidence"
    ]
}
```

### 7.2 Visualization Format

**Console Output:**
```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
KOLAM CLASSIFICATION RESULT
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Predicted Class: Pulli Kolam
Confidence: 78.5% [HIGH] âœ“

Confidence Breakdown:
  CNN Probability:     82.0% â†’ 53.3% (weight: 0.65)
  Rule Validation:     74.0% â†’ 25.9% (weight: 0.35)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Base Confidence:     79.2%
  Entropy Penalty:    -0.7%
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Final Confidence:    78.5%

Decision: AUTO-ACCEPT
```

---

## 8. BEHAVIOR ON DIFFERENT SCENARIOS

### 8.1 Scenario 1: Clear, Well-Formed Pulli Kolam

**Input Characteristics:**
- Perfect dot grid visible
- Regular spacing
- High contrast
- No noise

**Expected Behavior:**
```
CNN Probability: 0.95
Rule Score: 0.92
Entropy: 0.15 (low)
â†’ C_final = (0.65Ã—0.95 + 0.35Ã—0.92) Ã— (1 - 0.2Ã—0.15)
         = 0.940 Ã— 0.97
         = 91.2% [VERY HIGH]
Action: AUTO-ACCEPT
```

### 8.2 Scenario 2: Ambiguous Pattern (Pulli vs Chukku)

**Input Characteristics:**
- Dots present but connected by loops
- Mixed features from both types
- Moderate quality

**Expected Behavior:**
```
CNN Probability: 0.58 (uncertain)
Probabilities: [0.58, 0.35, 0.05, 0.02]
Rule Score: 0.62 (some rules pass, some fail)
Entropy: 0.82 (high)
â†’ C_final = (0.65Ã—0.58 + 0.35Ã—0.62) Ã— (1 - 0.2Ã—0.82)
         = 0.594 Ã— 0.836
         = 49.7% [LOW]
Action: REQUIRE HUMAN REVIEW
```

### 8.3 Scenario 3: Poor Quality / Incomplete Image

**Input Characteristics:**
- Partial pattern visible
- Low resolution
- Blur or noise
- Missing structural elements

**Expected Behavior:**
```
CNN Probability: 0.45 (weak signal)
Probabilities: [0.45, 0.28, 0.17, 0.10]
Rule Score: 0.35 (many rules fail)
Entropy: 1.15 (very high)
â†’ C_final = (0.65Ã—0.45 + 0.35Ã—0.35) Ã— (1 - 0.2Ã—1.15)
         = 0.415 Ã— 0.77
         = 31.9% [VERY LOW]
Action: REJECT / MANUAL CLASSIFICATION
```

### 8.4 Scenario 4: Adversarial Case (CNN Confident, Rules Disagree)

**Input Characteristics:**
- Pattern resembles training data superficially
- Violates geometric constraints
- Out-of-distribution features

**Expected Behavior:**
```
CNN Probability: 0.88 (overconfident)
Rule Score: 0.25 (rules strongly disagree)
Entropy: 0.35 (moderate)
â†’ C_final = (0.65Ã—0.88 + 0.35Ã—0.25) Ã— (1 - 0.2Ã—0.35)
         = 0.660 Ã— 0.93
         = 61.4% [MEDIUM]
Action: FLAG FOR REVIEW
Reason: CNN-Rule disagreement detected
```

### 8.5 Behavior Summary Table

| Scenario | P_cnn | S_rule | H_norm | C_final | Level | Action |
|----------|-------|--------|--------|---------|-------|--------|
| Perfect Pulli | 0.95 | 0.92 | 0.15 | 91% | Very High | Auto-accept |
| Good Line Kolam | 0.84 | 0.78 | 0.38 | 78% | High | Auto-accept |
| Ambiguous | 0.58 | 0.62 | 0.82 | 50% | Low | Human review |
| Poor quality | 0.45 | 0.35 | 1.15 | 32% | Very Low | Reject |
| CNN/Rule conflict | 0.88 | 0.25 | 0.35 | 61% | Medium | Flag for review |

---

## 9. OVERCONFIDENCE DETECTION

### 9.1 Detection Mechanisms

**Mechanism 1: CNN-Rule Disagreement**
```python
if abs(P_cnn - S_rule) > 0.30:
    warning = "Significant disagreement between CNN and rules"
    confidence_adjustment = -0.10  # Penalty
```

**Mechanism 2: Low Rule Score with High CNN**
```python
if P_cnn > 0.85 and S_rule < 0.50:
    warning = "Possible out-of-distribution sample"
    confidence_adjustment = -0.15
```

**Mechanism 3: High Entropy with High Probability**
```python
if P_cnn > 0.80 and H_norm > 0.70:
    warning = "Conflicting probability distribution"
    confidence_adjustment = -0.08
```

### 9.2 Sanity Check: Calibration Monitor

Continuously track on validation/test data:

```python
sanity_checks = {
    "probability_bins": [0.0-0.1, 0.1-0.2, ..., 0.9-1.0],
    "expected_accuracy": [0.05, 0.15, ..., 0.95],
    "actual_accuracy": [0.03, 0.18, ..., 0.93],
    "calibration_error": 0.042,
    "status": "WELL_CALIBRATED" if ECE < 0.05 else "NEEDS_RECALIBRATION"
}
```

### 9.3 Alert Conditions

| Condition | Threshold | Alert Level |
|-----------|-----------|-------------|
| ECE > 0.05 | Moderate miscalibration | âš ï¸ Warning |
| ECE > 0.10 | Severe miscalibration | ðŸš¨ Critical |
| CNN-Rule disagreement > 30% of cases | Systemic issue | ðŸš¨ Critical |
| Very Low confidence > 20% of cases | Data quality issue | âš ï¸ Warning |

---

## 10. MATHEMATICAL PROPERTIES & GUARANTEES

### 10.1 Formal Properties

**Property 1: Monotonicity**
```
If P_cnnâ†‘ and S_rule constant â†’ C_finalâ†‘
If S_ruleâ†‘ and P_cnn constant â†’ C_finalâ†‘
If H_normâ†‘ â†’ C_finalâ†“
```

**Property 2: Boundary Conditions**
```
C_final(1, 1, 0) = 1.0 (perfect case)
C_final(0, 0, 1) = 0.0 (worst case)
```

**Property 3: Lipschitz Continuity**
```
|C_final(xâ‚) - C_final(xâ‚‚)| â‰¤ L Ã— ||xâ‚ - xâ‚‚||
System is stable to small input perturbations
```

### 10.2 Sensitivity Analysis

```python
# Partial derivatives (at nominal point P=0.8, S=0.7, H=0.3)
âˆ‚C/âˆ‚P_cnn â‰ˆ +0.61   # CNN has strong influence
âˆ‚C/âˆ‚S_rule â‰ˆ +0.33  # Rules moderate influence
âˆ‚C/âˆ‚H_norm â‰ˆ -0.15  # Entropy penalty effect
```

---

## 11. IMPLEMENTATION ARCHITECTURE

### 11.1 Module Structure
```
scripts/confidence_scoring/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ confidence_calculator.py   # Core confidence computation
â”œâ”€â”€ entropy_analyzer.py         # Entropy and consistency metrics
â”œâ”€â”€ calibration_monitor.py      # Calibration tracking and validation
â”œâ”€â”€ threshold_manager.py        # Dynamic threshold management
â”œâ”€â”€ explainer.py                # Explanation generation
â””â”€â”€ validator.py                # Overconfidence detection
```

### 11.2 Integration Points

```python
# In inference pipeline
features = extract_features(image)
cnn_probs = cnn_model.predict(features)
rule_score = rule_validator.validate(cnn_pred, features)

# NEW: Enhanced confidence scoring
confidence_result = confidence_calculator.compute(
    cnn_probs=cnn_probs,
    rule_score=rule_score,
    return_explanation=True
)

# Use confidence for decision making
if confidence_result['confidence'] >= threshold_high:
    auto_accept(prediction)
elif confidence_result['confidence'] >= threshold_medium:
    flag_for_review(prediction)
else:
    reject_or_manual(prediction)
```

---

## 12. VALIDATION STRATEGY

### 12.1 Unit Tests
- âœ… Test boundary conditions (0, 1 inputs)
- âœ… Test parameter ranges (Î±, Î², Î³)
- âœ… Test mathematical properties
- âœ… Test threshold logic

### 12.2 Integration Tests
- âœ… Test with real image features
- âœ… Verify calibration on validation set
- âœ… Check consistency across runs
- âœ… Validate explanation generation

### 12.3 Empirical Validation
- âœ… Measure ECE on test set
- âœ… Analyze confidence distributions
- âœ… Check error rates per confidence level
- âœ… Compare with baseline methods

---

## 13. REFERENCES & FURTHER READING

1. **Guo et al. (2017):** "On Calibration of Modern Neural Networks" â€” Temperature scaling
2. **Platt (1999):** "Probabilistic Outputs for SVMs" â€” Platt scaling
3. **Shannon (1948):** "A Mathematical Theory of Communication" â€” Entropy definition
4. **Niculescu-Mizil & Caruana (2005):** "Predicting Good Probabilities with Supervised Learning"
5. **Kumar et al. (2019):** "Verified Uncertainty Calibration"

---

## 14. CONCLUSION

This confidence scoring design:
- âœ… Combines three independent evidence sources
- âœ… Provides interpretable, explainable scores
- âœ… Includes calibration monitoring
- âœ… Handles ambiguous and adversarial cases gracefully
- âœ… Supports human-in-the-loop workflows
- âœ… Maintains mathematical rigor

**Next Steps:**
1. Implement core modules
2. Run calibration experiments on validation data
3. Tune Î±, Î², Î³ parameters
4. Deploy in inference pipeline
5. Monitor performance in production

---

**Document Status:** âœ… COMPLETE  
**Ready for Implementation:** YES
